# Cyber-Inference Docker Compose Configuration (CPU)
#
# Usage:
#   docker-compose up -d          # Start in background
#   docker-compose logs -f        # View logs
#   docker-compose down           # Stop and remove containers

services:
  cyber-inference:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cyber-inference
    restart: unless-stopped

    ports:
      - "8337:8337"

    volumes:
      # Persist data and models across container restarts
      - cyber-data:/app/data
      - cyber-models:/app/models

    environment:
      # Server configuration
      - CYBER_INFERENCE_HOST=0.0.0.0
      - CYBER_INFERENCE_PORT=8337
      - CYBER_INFERENCE_LOG_LEVEL=INFO

      # Model settings
      - CYBER_INFERENCE_DEFAULT_CONTEXT_SIZE=4096
      - CYBER_INFERENCE_MAX_CONTEXT_SIZE=32768
      - CYBER_INFERENCE_MODEL_IDLE_TIMEOUT=300
      - CYBER_INFERENCE_MAX_LOADED_MODELS=2

      # Resource limits
      - CYBER_INFERENCE_MAX_MEMORY_PERCENT=80

      # Security (uncomment to enable)
      # - CYBER_INFERENCE_ADMIN_PASSWORD=your-secure-password

      # HuggingFace (uncomment if needed for private models)
      # - CYBER_INFERENCE_HF_TOKEN=hf_xxxxxxxxxxxxx

    # Resource limits
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8337/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

volumes:
  cyber-data:
    name: cyber-inference-data
  cyber-models:
    name: cyber-inference-models
