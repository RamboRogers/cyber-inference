# Cyber-Inference Docker Compose Configuration
#
# Usage:
#   docker-compose up -d          # Start in background
#   docker-compose logs -f        # View logs
#   docker-compose down           # Stop and remove containers

version: '3.8'

services:
  cyber-inference:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cyber-inference
    restart: unless-stopped

    ports:
      - "8337:8337"

    volumes:
      # Persist data across container restarts
      - cyber-data:/app/data
      - cyber-models:/app/models
      - cyber-bin:/app/bin

    environment:
      # Server configuration
      - CYBER_INFERENCE_HOST=0.0.0.0
      - CYBER_INFERENCE_PORT=8337
      - CYBER_INFERENCE_LOG_LEVEL=INFO

      # Model settings
      - CYBER_INFERENCE_DEFAULT_CONTEXT_SIZE=4096
      - CYBER_INFERENCE_MAX_CONTEXT_SIZE=32768
      - CYBER_INFERENCE_MODEL_IDLE_TIMEOUT=300
      - CYBER_INFERENCE_MAX_LOADED_MODELS=2

      # Resource limits
      - CYBER_INFERENCE_MAX_MEMORY_PERCENT=80
      - CYBER_INFERENCE_LLAMA_GPU_LAYERS=-1

      # Security (uncomment to enable)
      # - CYBER_INFERENCE_ADMIN_PASSWORD=your-secure-password

      # HuggingFace (uncomment if needed for private models)
      # - CYBER_INFERENCE_HF_TOKEN=hf_xxxxxxxxxxxxx

    # Resource limits
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8337/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

# NVIDIA GPU Configuration (uncomment for GPU support)
#   cyber-inference-gpu:
#     extends:
#       service: cyber-inference
#     container_name: cyber-inference-gpu
#     runtime: nvidia
#     environment:
#       - NVIDIA_VISIBLE_DEVICES=all
#       - CYBER_INFERENCE_LLAMA_GPU_LAYERS=-1
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 1
#               capabilities: [gpu]

volumes:
  cyber-data:
    name: cyber-inference-data
  cyber-models:
    name: cyber-inference-models
  cyber-bin:
    name: cyber-inference-bin

