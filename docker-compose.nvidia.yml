# Cyber-Inference Docker Compose - NVIDIA GPU Configuration
#
# SGLang + CUDA PyTorch are installed automatically by start.sh at first run.
# llama.cpp is also available as a fallback engine.
#
# Usage:
#   docker-compose -f docker-compose.nvidia.yml up -d
#
# Requirements:
#   - NVIDIA Container Toolkit installed
#   - docker-compose v2.x with GPU support

services:
  cyber-inference-gpu:
    build:
      context: .
      dockerfile: Dockerfile.nvidia
    container_name: cyber-inference-gpu
    restart: unless-stopped

    # NVIDIA runtime
    runtime: nvidia

    ports:
      - "8337:8337"

    volumes:
      - cyber-data:/app/data
      - cyber-models:/app/models

    environment:
      # NVIDIA settings
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Server configuration
      - CYBER_INFERENCE_HOST=0.0.0.0
      - CYBER_INFERENCE_PORT=8337
      - CYBER_INFERENCE_LOG_LEVEL=INFO

      # GPU-optimized settings
      - CYBER_INFERENCE_LLAMA_GPU_LAYERS=-1
      - CYBER_INFERENCE_DEFAULT_CONTEXT_SIZE=8192
      - CYBER_INFERENCE_MAX_CONTEXT_SIZE=65536
      - CYBER_INFERENCE_MAX_LOADED_MODELS=1
      - CYBER_INFERENCE_MAX_MEMORY_PERCENT=90

      # SGLang settings (uncomment to customize)
      # - CYBER_INFERENCE_SGLANG_MEM_FRACTION=0.85
      # - CYBER_INFERENCE_SGLANG_TP_SIZE=1
      # - CYBER_INFERENCE_NO_SGLANG=0

      # Security (uncomment to enable)
      # - CYBER_INFERENCE_ADMIN_PASSWORD=your-secure-password

      # HuggingFace (uncomment if needed for private models)
      # - CYBER_INFERENCE_HF_TOKEN=hf_xxxxxxxxxxxxx

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 32G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8337/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s  # SGLang install on first run needs time

    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

volumes:
  cyber-data:
    name: cyber-inference-data
  cyber-models:
    name: cyber-inference-models
